{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Block Continuity CNN Training\n",
        "\n",
        "Train 1D CNN model for block continuity detection.\n",
        "\n",
        "**Workflow:**\n",
        "1. Mount Google Drive\n",
        "2. Load CSV dataset (64-dim features)\n",
        "3. Train CNN model\n",
        "4. Export to ONNX\n",
        "5. Download model\n",
        "\n",
        "**Requirements:**\n",
        "- Dataset CSV in Google Drive: `ML/datasets/continuity_dataset.csv`\n",
        "- GPU runtime recommended (Runtime > Change runtime type > T4 GPU)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install onnx onnxruntime tensorboard -q\n",
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define CNN Model"
      ],
      "metadata": {
        "id": "model_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "\n",
        "class ContinuityCNN1D(nn.Module):\n",
        "    \"\"\"\n",
        "    1D CNN for block continuity classification\n",
        "\n",
        "    Input: (batch, 64) feature vector\n",
        "    Output: (batch, 2) logits\n",
        "\n",
        "    Architecture:\n",
        "    - Reshape: (batch, 64) -> (batch, 1, 64)\n",
        "    - Conv1d layers: 32 -> 64 -> 128 channels\n",
        "    - Global Average Pooling\n",
        "    - FC classifier: 128 -> 64 -> 2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int = 64,\n",
        "        num_classes: int = 2,\n",
        "        channels: List[int] = None,\n",
        "        kernel_sizes: List[int] = None,\n",
        "        dropout: float = 0.3,\n",
        "        use_batch_norm: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if channels is None:\n",
        "            channels = [32, 64, 128]\n",
        "        if kernel_sizes is None:\n",
        "            kernel_sizes = [5, 3, 3]\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Build conv layers\n",
        "        conv_layers = []\n",
        "        in_channels = 1\n",
        "\n",
        "        for out_channels, kernel_size in zip(channels, kernel_sizes):\n",
        "            padding = kernel_size // 2\n",
        "            conv_layers.append(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
        "            )\n",
        "            if use_batch_norm:\n",
        "                conv_layers.append(nn.BatchNorm1d(out_channels))\n",
        "            conv_layers.append(nn.ReLU(inplace=True))\n",
        "            conv_layers.append(nn.Dropout(dropout))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*conv_layers)\n",
        "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(channels[-1], 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.unsqueeze(1)  # (batch, 1, 64)\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test model\n",
        "model = ContinuityCNN1D()\n",
        "test_input = torch.randn(32, 64)\n",
        "output = model(test_input)\n",
        "print(f\"Model created successfully!\")\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "define_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Dataset"
      ],
      "metadata": {
        "id": "dataset_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration\n",
        "DATASET_PATH = \"/content/drive/MyDrive/ML/datasets/continuity_dataset.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/ML/models/continuity\"\n",
        "\n",
        "# Create output directory\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "class ContinuityDataset(Dataset):\n",
        "    \"\"\"Dataset for continuity features\"\"\"\n",
        "\n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def load_dataset(csv_path: str, test_size: float = 0.2):\n",
        "    \"\"\"Load and split dataset\"\"\"\n",
        "    print(f\"Loading dataset from {csv_path}...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "\n",
        "    # Extract features (columns f0-f63)\n",
        "    feature_cols = [f\"f{i}\" for i in range(64)]\n",
        "    X = df[feature_cols].values.astype(np.float32)\n",
        "\n",
        "    # Extract labels\n",
        "    y = df[\"is_continuous\"].values.astype(np.int64)\n",
        "\n",
        "    # Print class distribution\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    print(f\"Class distribution:\")\n",
        "    for label, count in zip(unique, counts):\n",
        "        print(f\"  Class {label}: {count} ({count/len(y)*100:.1f}%)\")\n",
        "\n",
        "    # Split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(X_train)}\")\n",
        "    print(f\"Val samples: {len(X_val)}\")\n",
        "\n",
        "    # Normalize features\n",
        "    mean = X_train.mean(axis=0)\n",
        "    std = X_train.std(axis=0) + 1e-8\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_val = (X_val - mean) / std\n",
        "\n",
        "    # Save normalization params\n",
        "    norm_params = {\"mean\": mean.tolist(), \"std\": std.tolist()}\n",
        "\n",
        "    return (\n",
        "        ContinuityDataset(X_train, y_train),\n",
        "        ContinuityDataset(X_val, y_val),\n",
        "        norm_params\n",
        "    )\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "train_dataset, val_dataset, norm_params = load_dataset(DATASET_PATH)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "config = {\n",
        "    \"epochs\": 100,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"patience\": 15,  # Early stopping patience\n",
        "    \"use_amp\": True,  # Mixed precision training\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ContinuityCNN1D().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=config[\"epochs\"]\n",
        ")\n",
        "\n",
        "# AMP scaler\n",
        "scaler = torch.amp.GradScaler() if config[\"use_amp\"] else None\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Config: {config}\")"
      ],
      "metadata": {
        "id": "training_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train Model"
      ],
      "metadata": {
        "id": "train_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for features, labels in loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler:\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * features.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for features, labels in loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item() * features.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "# Training loop\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, scaler, device\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Record history\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_acc\"].append(train_acc)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    # Print progress\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1:3d}/{config['epochs']} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "          f\"LR: {lr:.2e}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"config\": config,\n",
        "            \"norm_params\": norm_params\n",
        "        }\n",
        "        torch.save(checkpoint, f\"{OUTPUT_DIR}/best_cnn.pt\")\n",
        "        print(f\"  >> Saved best model (val_acc: {val_acc:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config[\"patience\"]:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training complete! Best val accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualize Training"
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history[\"train_loss\"], label=\"Train\")\n",
        "axes[0].plot(history[\"val_loss\"], label=\"Validation\")\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].set_title(\"Training and Validation Loss\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history[\"train_acc\"], label=\"Train\")\n",
        "axes[1].plot(history[\"val_acc\"], label=\"Validation\")\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].set_title(\"Training and Validation Accuracy\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/training_curves.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTraining curves saved to {OUTPUT_DIR}/training_curves.png\")"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Export to ONNX"
      ],
      "metadata": {
        "id": "export_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Load best checkpoint\n",
        "checkpoint = torch.load(f\"{OUTPUT_DIR}/best_cnn.pt\", weights_only=False)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "print(f\"Validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 64).to(device)\n",
        "onnx_path = f\"{OUTPUT_DIR}/continuity_cnn.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['features'],\n",
        "    output_names=['logits'],\n",
        "    dynamic_axes={\n",
        "        'features': {0: 'batch_size'},\n",
        "        'logits': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "# Verify ONNX model\n",
        "onnx_model = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"input_dim\": 64,\n",
        "    \"num_classes\": 2,\n",
        "    \"model_type\": \"cnn1d\",\n",
        "    \"norm_params\": checkpoint[\"norm_params\"],\n",
        "    \"val_accuracy\": checkpoint[\"val_acc\"],\n",
        "    \"training_epochs\": checkpoint[\"epoch\"],\n",
        "    \"label_map\": {\"0\": \"discontinuous\", \"1\": \"continuous\"}\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/continuity_cnn.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# Get file sizes\n",
        "onnx_size = os.path.getsize(onnx_path)\n",
        "print(f\"\\nONNX model exported: {onnx_path}\")\n",
        "print(f\"Model size: {onnx_size / 1024:.1f} KB\")\n",
        "print(f\"Metadata saved: {OUTPUT_DIR}/continuity_cnn.json\")"
      ],
      "metadata": {
        "id": "export_onnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Verify ONNX Inference"
      ],
      "metadata": {
        "id": "verify_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "# Create ONNX Runtime session\n",
        "session = ort.InferenceSession(onnx_path)\n",
        "\n",
        "# Test inference\n",
        "test_features = np.random.randn(10, 64).astype(np.float32)\n",
        "ort_outputs = session.run(None, {'features': test_features})\n",
        "\n",
        "print(f\"ONNX Runtime inference test:\")\n",
        "print(f\"  Input shape: {test_features.shape}\")\n",
        "print(f\"  Output shape: {ort_outputs[0].shape}\")\n",
        "\n",
        "# Compare with PyTorch output\n",
        "model.cpu()\n",
        "with torch.no_grad():\n",
        "    pt_output = model(torch.from_numpy(test_features)).numpy()\n",
        "\n",
        "max_diff = np.abs(ort_outputs[0] - pt_output).max()\n",
        "print(f\"  Max difference from PyTorch: {max_diff:.2e}\")\n",
        "\n",
        "if max_diff < 1e-5:\n",
        "    print(\"  ONNX export verified successfully!\")\n",
        "else:\n",
        "    print(\"  Warning: Larger than expected difference\")"
      ],
      "metadata": {
        "id": "verify_onnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Download Models\n",
        "\n",
        "The trained models are saved to Google Drive at:\n",
        "- `ML/models/continuity/continuity_cnn.onnx` - ONNX model for C++ inference\n",
        "- `ML/models/continuity/continuity_cnn.json` - Metadata with normalization params\n",
        "- `ML/models/continuity/best_cnn.pt` - PyTorch checkpoint\n",
        "\n",
        "You can also download directly:"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Output files:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    path = os.path.join(OUTPUT_DIR, f)\n",
        "    size = os.path.getsize(path)\n",
        "    print(f\"  {f}: {size/1024:.1f} KB\")\n",
        "\n",
        "# Download ONNX model (uncomment to download)\n",
        "# files.download(f\"{OUTPUT_DIR}/continuity_cnn.onnx\")\n",
        "# files.download(f\"{OUTPUT_DIR}/continuity_cnn.json\")"
      ],
      "metadata": {
        "id": "download_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage in C++\n",
        "\n",
        "After downloading, copy files to your C++ project:\n",
        "\n",
        "```bash\n",
        "# Copy to Filerestore_CLI models directory\n",
        "copy continuity_cnn.onnx Filerestore_CLI\\x64\\Release\\models\\continuity\\\n",
        "copy continuity_cnn.json Filerestore_CLI\\x64\\Release\\models\\continuity\\\n",
        "```\n",
        "\n",
        "The `BlockContinuityDetector` class will automatically load the new model."
      ],
      "metadata": {
        "id": "cpp_usage"
      }
    }
  ]
}
